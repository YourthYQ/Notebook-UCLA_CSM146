## Underfitting vs. Overfitting

At a high level, both underfitting and overfitting are related to how well a model learns from the training data and how effectively it applies that learning to new, unseen data.

- **Underfitting**: Occurs when a model is too simple to capture the underlying structure of the data. It fails to learn the significant patterns and relationships within the dataset, leading to poor performance on both training and test data.

- **Overfitting**: Happens when a model is excessively complex and captures even the noise or random fluctuations in the training data. While it performs exceptionally well on training data, it struggles to generalize to new examples, resulting in poor test performance.

Both scenarios result in inaccurate predictions, but for different reasons.

---

## Bias & Variance

A key concept that helps explain underfitting and overfitting is the **bias-variance tradeoff**:

In machine learning, **bias** and **variance** are two important concepts that measure model performance.  
They together affect the **generalization ability** of the model.

### 1. Bias (偏差)

- **Bias** refers to the **systematic error** between the predicted values and the actual values.
- High bias means that the model has **poor fitting ability**, usually because the model is **too simple** and cannot capture complex patterns in the data.
- **High bias cases** are known as **underfitting**.

> **Example of High Bias**
>
> Suppose we use **linear regression** to fit a nonlinear dataset (e.g., a quadratic function). Since the model is too simple, it cannot capture the pattern well, leading to large errors.
>
> - **True relationship**:  
  \[
  y = x^2 + 5x + 3
  \]
> - **Linear regression model**:  
  \[
  y = ax + b
  \]
  (model is too simple)
> - **Result**: Large errors, model cannot fit the data correctly.

### 2. Variance (方差)

- **Variance** refers to the **sensitivity of the model** to different training datasets, meaning how much the model's predictions fluctuate across different datasets.
- High variance means that the model **overfits the training data**, learning noise instead of patterns, leading to **poor generalization** on new data.
- **High variance cases** are known as **overfitting**.

> **Example of High Variance**
> 
> If we use **high-degree polynomial regression** (e.g., a **9th-degree polynomial**) to fit the data, the model may strictly follow the training data details (including noise) but perform poorly on test data.
> 
> - **True relationship**:  
  \[
  y = x^2 + 5x + 3
  \]
> - **9th-degree polynomial model**:  
  \[
  y = a_1 x^9 + a_2 x^8 + \dots + a_9 x + a_{10}
  \]
  (model is too complex)
> - **Result**: Model fits the training data well but has large errors on new data.

### 3. Bias-Variance Tradeoff

#### Bias-Variance Decomposition Formula

The total error (expected mean squared error) of a model can be expressed as:

\[
\mathbb{E}[(y - \hat{y})^2] = \text{Bias}^2 + \text{Variance} + \sigma^2
\]

where:

- **\(\mathbb{E}[(y - \hat{y})^2]\)** = **Total Error** (Expected Mean Squared Error)
- **\(\sigma^2\)** = **Irreducible error** (noise in the data that no model can eliminate).

#### Tradeoff
- If **Bias** is high → The model is too simple → **Underfitting**.
- If **Variance** is high → The model is too complex → **Overfitting**.
- **The best model** minimizes both **Bias** and **Variance**, while recognizing that \(\sigma^2\) is unavoidable.


### 4. How to Reduce Bias and Variance?

| **Problem**                | **Solution**  |
|----------------------------|--------------|
| **High Bias (Underfitting)** | - Use a more complex model (e.g., add polynomial features) <br> - Train for a longer time <br> - Choose a smaller regularization parameter |
| **High Variance (Overfitting)** | - Collect more training data <br> - Use regularization (e.g., L1/L2 regularization) <br> - Use a simpler model |

| Prevention Techniques            | Underfitting       | Overfitting       |
|----------------------------------|--------------------|-------------------|
| Increase model complexity        | ✅ Helps           | ❌ Worsens       |
| Increase training data           | ✅ Helps           | ✅ Helps          |
| Increase training attributes     | ✅ Helps           | ❌ Worsens        |
| Regularization (L1, L2, Dropout) | ❌ Worsens         | ✅ Helps          |
| Cross-validation                 | ✅ Helps           | ✅ Helps          |

The ideal model achieves a balance between bias and variance, ensuring strong generalization to new data while avoiding both **oversimplification** and **memorization**.