## Recalling to Perceptrons and Learning

A perceptron is one of the simplest types of neural networks, primarily used for binary classification. It operates by finding a decision boundary (a hyperplane) that separates data into two classes. Over iterations, it updates its weights to better classify the training data. Finally, it returns **the last weight vector** as the final model.

However, certain challenges exist:

1. **Vanilla Perceptron**:  
   While straightforward, it might miss **intermediate weight vectors** that performed better than the last weight vector during training.

2. **Data Non-Separability**:  
   When data is not linearly separable, performance may degrade.

To address these issues, techniques like **voting** and **averaging** are introduced.

---

## Voting Perceptron

### Concept

The vanilla perceptron updates weights iteratively and eventually returns the last weight vector as the final model. However:
- During training, the perceptron may encounter **weight vectors that classify the data better than the last weight vector** but are replaced by later updates.
- A **voting mechanism** remembers all intermediate weight vectors and allows them to contribute during the final prediction.

### How Voting Works

**1. Track Updates**
- During training, the perceptron updates its weight vector \( w \) whenever it misclassifies a training sample.
- Each updated weight vector \( w_i \) is **stored** along with its **survival count (votes) \( \alpha_i \)**, which represents the number of iterations it remained unchanged before being replaced by a new weight vector.

This step ensures that we preserve the knowledge gained at different stages of training, allowing each weight vector to contribute to the final prediction.

**2. Weighted Contribution During Prediction**
For a new input \( x \), each stored weight vector \( w_i \) contributes to the final prediction as follows:
- **Compute the dot product \( w_i^T x \)**:
  - This measures how strongly \( x \) aligns with the decision boundary defined by \( w_i \). The sign of \( w_i^T x \) determines whether \( x \) is classified as positive or negative by \( w_i \).
- **Weight the contribution by \( \alpha_i \)**:
  - Multiply \( w_i^T x \) by the survival count \( \alpha_i \), which acts as the "weight" of \( w_i \)'s vote. Weight vectors that survived longer contribute more heavily.

The contributions from all stored weight vectors are aggregated to compute the overall decision score:
\[
\sum_{i=1}^k \alpha_i (w_i^T x)
\]
Here:
- \( k \): Total number of stored weight vectors.
- \( \alpha_i \): Survival count of the \( i \)-th weight vector.

**3. Combined Weight Vector \( \tilde{w} \)**
To simplify prediction, the algorithm computes a **combined weight vector** \( \tilde{w} \) as the weighted sum of all stored weight vectors:
\[
\tilde{w} = \alpha_1 w_1 + \alpha_2 w_2 + \alpha_3 w_3 + \dots
\]
Where:
- \( w_i \): Intermediate weight vectors encountered during training.
- \( \alpha_i \): The number of iterations each weight vector survived before being replaced.

This combined weight vector \( \tilde{w} \) encapsulates the voting process into a single vector, avoiding the need to compute the weighted sum explicitly during prediction.

**4. Final Prediction**
Once \( \tilde{w} \) is computed, the final prediction for a new input \( x \) is determined by:
\[
\hat{y} = \text{sign}(\tilde{w}^T x)
\]
This step involves:
1. Computing the dot product \( \tilde{w}^T x \), which gives the overall decision score.
2. Taking the sign of the score to determine the predicted class:
   - \( \hat{y} = +1 \): If \( \tilde{w}^T x > 0 \).
   - \( \hat{y} = -1 \): If \( \tilde{w}^T x < 0 \).

---

## Averaged Perceptron

### Concept

Instead of storing all intermediate weights, the averaged perceptron calculates the **average** of all weight vectors seen during training.  
Longer surviving vectors contribute more to the average, reflecting their significance.

### How Averaging Works

1. **Update Rule**:  
   For each iteration, add the current weight vector \( \mathbf{w} \) to the **cumulative sum \( \boldsymbol{\mu} \)** (the averaged weight vector).

2. **Final Prediction**:  
   Use the sign of \( \boldsymbol{\mu}^\top \mathbf{x} \) for prediction.

### Algorithm

1. **Initialization**:
   - Set \( \mathbf{w} = 0 \) (initial weight vector).
   - Set \( \boldsymbol{\mu} = 0 \) (accumulated weight vector).

2. **Training**:
   - For each data point \( (\mathbf{x}, y) \):
     - Compute the dot product \( a = \mathbf{w}^\top \mathbf{x} \).
     - If \( a \cdot y \leq 0 \), update:
       \[
       \mathbf{w} = \mathbf{w} + y\mathbf{x}
       \]
       - Update the cumulative weight:
         \[
         \boldsymbol{\mu} = \boldsymbol{\mu} + \mathbf{w}
         \]

3. **Return \( \boldsymbol{\mu} \)** as the final model.

4. **Final prediction**:
\[
\text{Prediction} = \text{sign}(\boldsymbol{\mu}^\top \mathbf{x})
\]

---

## Comparison: Voting vs. Averaging

| **Aspect**         | **Voting Perceptron**                         | **Averaged Perceptron**                       |
|---------------------|-----------------------------------------------|-----------------------------------------------|
| **Storage**         | Stores all intermediate weight vectors        | Only stores one cumulative vector \( \boldsymbol{\mu} \) |
| **Accuracy**        | Slightly better generalization due to weighted voting | Effective and practical for real-world use    |
| **Complexity**      | Higher due to storage and computation         | Lower, more efficient                         |
| **Use Case**        | Theoretical studies, small datasets           | Large datasets, practical applications        |

## Applications in Real-World Problems

1. **Text Classification**:  
   - Averaged perceptron is widely used in natural language processing tasks, such as sentiment analysis or spam detection, due to its efficiency.

2. **Image Recognition**:  
   - Voting perceptron may be preferred in research scenarios where interpretability and generalization are critical.

3. **Online Learning**:  
   - Both techniques are suitable for systems that learn from streaming data.
