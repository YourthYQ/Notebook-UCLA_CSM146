# Linear Regression

Linear regression is one of the simplest and most fundamental techniques in machine learning and statistics. It is used to model the relationship between a **dependent variable** (output) and one or more **independent variables** (inputs).

#### Univariate Linear Regression

When we have only one input variable \( x \), the relationship between the dependent variable \( y \) and \( x \) can be expressed as:

\[
y = \theta_0 + \theta_1 x + \epsilon
\]

where:

- \( \theta_0 \) (**intercept**) and \( \theta_1 \) (**slope**) are the **parameters** of the model.
- \( \epsilon \) is an **error term** that accounts for any noise in the data.

The goal is to **find the optimal values of** \( \theta_0 \) and \( \theta_1 \) that minimize the difference between the **predicted and actual values**.

#### Cost Function (Residual Sum of Squares - RSS)

To quantify the accuracy of the model, we define a **cost function**, called the **Residual Sum of Squares (RSS)**:

\[
J(\theta) = \sum_n (y_n - h_{\theta}(x_n))^2
\]

where:

- \( h_{\theta}(x_n) \) is the **predicted value**, given by \( \theta_0 + \theta_1 x_n \).
- \( y_n \) is the **actual observed value**.

This function **measures the squared difference** between actual and predicted values. The goal is to **find values of** \( \theta_0 \) and \( \theta_1 \) **that minimize this cost function**.

#### Finding the Optimal Parameters - Analytical Solution

To find the parameters that minimize \( J(\theta) \), we take the derivative of \( J(\theta) \) with respect to \( \theta_0 \) and \( \theta_1 \) and **set them to zero**.

##### Derivatives:

\[
\frac{\partial J}{\partial \theta_0} = -2 \sum_n (y_n - (\theta_0 + \theta_1 x_n)) = 0
\]

\[
\frac{\partial J}{\partial \theta_1} = -2 \sum_n (y_n - (\theta_0 + \theta_1 x_n)) x_n = 0
\]

By solving these equations, we derive the **Normal Equations**:

\[
\theta_1 = \frac{\sum (x_n - \bar{x})(y_n - \bar{y})}{\sum (x_n - \bar{x})^2}
\]

\[
\theta_0 = \bar{y} - \theta_1 \bar{x}
\]

where \( \bar{x} \) and \( \bar{y} \) are the **mean values** of \( x \) and \( y \).

---

## Probabilistic Interpretation

Linear regression can also be **interpreted probabilistically**. We assume that the output \( Y \) follows a **normal (Gaussian) distribution**:

\[
Y = \theta_0 + \theta_1 X + \eta, \quad \text{where} \quad \eta \sim N(0, \sigma^2)
\]

This means that given \( X \), the distribution of \( Y \) follows a normal distribution centered at \( \theta_0 + \theta_1 X \).

#### Maximum Likelihood Estimation

To estimate \( \theta_0 \) and \( \theta_1 \), we **maximize the likelihood function**:

\[
p(y_n | x_n; \theta) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{(y_n - (\theta_0 + \theta_1 x_n))^2}{2 \sigma^2} \right)
\]

The **log-likelihood** of the data:

\[
LL(\theta) = \sum_n \log p(y_n | x_n; \theta)
\]

\[
= -\frac{1}{2\sigma^2} \sum_n (y_n - (\theta_0 + \theta_1 x_n))^2 - \frac{N}{2} \log (2 \pi \sigma^2)
\]

By **maximizing** this log-likelihood, we obtain the **same solution as the Normal Equations** derived earlier.

**Conclusion**: Minimizing the RSS (cost function) is **equivalent to maximizing the likelihood** under the Gaussian noise assumption.

#### Multivariate Linear Regression

When the input data has **multiple features (dimensions)**, we **generalize** the model:

\[
y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_D x_D
\]

or in **vector notation**:

\[
y = \theta^T x
\]

where:

- \( x = [1, x_1, x_2, ..., x_D]^T \) (**augmented with a 1** for the bias term)
- \( \theta = [\theta_0, \theta_1, ..., \theta_D]^T \)

---

## Normal Equations

The **Normal Equation** is a closed-form solution for finding the optimal parameters \( \theta \) in **linear regression** without requiring iterative methods like **Gradient Descent**.

The **Normal Equation exists** because the cost function in linear regression has a unique **global minimum** that can be found algebraically. This is due to the properties of **convexity** and **differentiability** in the cost function.

#### Generating Normal Equations

Recall that in **linear regression**, we define the cost function as:

\[
J(\theta) = \| y -X\theta \|^2_2 = \sum_{i=1}^{N} (h_{\theta}(x_i) - y_i)^2
\]


The cost function in matrix form is:

\[
J(\theta) = (y -X\theta)^T (y -X\theta)
\]

Expanding this:

\[
\begin{aligned}
J(\theta) &= (y -X\theta)^T (y -X\theta) \\
          &= (y^T - \{X\theta\}^T)(y - X\theta) \\
          &= (y^T - \theta^T X^T)(y - X\theta) \\
          &= (y^T)(y - X\theta) - \theta^T X^T (y - X\theta) \\
          &= y^T y - y^T X\theta - \theta^T X^T y + \theta^T X^T X \theta \\
          &= \text{constant} - 2 y^T X\theta + \theta^T X^T X \theta
\end{aligned}
\]



To minimize \( J(\theta) \), we compute the derivative with respect to \( \theta \):

\[
\nabla J(\theta) = 2 X^T X \theta - 2 X^T y
\]

Setting this derivative to zero:

\[
2 X^T X \theta - 2 X^T y = 0
\]

Solve for \( \theta \):

\[
\theta = (X^T X)^{-1} X^T y
\]

This is the **Normal Equation**.

#### Time Complexity of Normal Equations

To compute \( \theta \), we need to:

- `Matrix Multiplication` \( X^T X \) \( \rightarrow \mathcal{O}(ND^2) \)
- `Matrix Inversion` \( (X^T X)^{-1} \) \( \rightarrow \mathcal{O}(D^3) \)

For very large datasets, this becomes computationally expensive.

---

## Optimization via Gradient Descent

Instead of inverting \( X^T X \), we can use **Gradient Descent** to iteratively find the optimal \( \theta \).

#### (Batch) Gradient Descent

\[
\theta^{(t+1)} = \theta^{(t)} - \eta \nabla J(\theta^{(t)})
\]

where:

- \( \eta \) is the **learning rate**.
- \( \nabla J(\theta) \) is the **gradient of the loss function**.

Gradient Descent guarantees **convergence** to the minimum because \( J(\theta) \) is convex.

#### Stochastic Gradient Descent (SGD)

Instead of computing the gradient using all data points, **SGD** updates \( \theta \) for each data point:

\[
\theta^{(t+1)} = \theta^{(t)} - \eta (x_n^T \theta^{(t)} - y_n) x_n
\]

- **SGD is faster** \( (\mathcal{O}(D)) \) per update compared to full gradient descent \( (\mathcal{O}(ND)) \).
- **Mini-batch gradient descent** combines elements of both.
