### Before Note: Logic behind Kernelized SVM

**\(\text{Primal (Soft) SVM} \Rightarrow \text{Dual SVM} \Rightarrow \text{Kernelized Dual SVM}\)**

1. **\(\text{Primal (Soft) SVM}\)**: Primal SVM is formulated to handle **linear classification (Hard SVM)** but cannot efficiently deal with **nonlinear decision boundaries (Soft SVM)**.

2. **\(\text{Dual SVM}\)**: To overcome this limitation, we derive the **dual formulation**, which expresses the optimization problem in terms of **dot products** between training samples.

3. **\(\text{Kernelized Dual SVM}\)**: Since the dual formulation depends only on **dot products**, we apply the **kernel trick**, replacing \( x_m^T x_n \) with a **kernel function** \( k(x_m, x_n) \). This allows SVM to **model nonlinear decision boundaries efficiently** without explicitly transforming features.

---

## Recap: Primal Formulation of SVM

#### 1. Problem Setup
Given a dataset with \( N \) samples \( \{(x_n, y_n)\}_{n=1}^{N} \), where:
- \( x_n \in \mathbb{R}^d \) are the input features.
- \( y_n \in \{-1, +1\} \) are the class labels.

The goal of SVM is to **find a hyperplane that maximizes the margin** between the two classes.

#### 1.2 Hard-Margin SVM (Linearly Separable Case)

If the data is **perfectly separable**, we aim to find \( w \) and \( b \) such that:

\[
y_n (w^T x_n + b) \geq 1, \quad \forall n
\]

The **primal optimization problem** for hard-margin SVM is:

\[
\min_{w, b} \quad \frac{1}{2} \|w\|^2
\]

subject to:

\[
y_n (w^T x_n + b) \geq 1, \quad \forall n
\]

- The objective function \( \frac{1}{2} \|w\|^2 \) represents the **margin size**, which we aim to **maximize**.
- The constraint ensures **correct classification with a margin of at least 1**.

#### 1.3 Soft-Margin SVM (Non-Separable Case)

For real-world data that is **not perfectly separable**, we introduce **slack variables** \( \xi_n \) to allow some misclassification:

\[
y_n (w^T x_n + b) \geq 1 - \xi_n, \quad \xi_n \geq 0
\]

The **primal optimization problem** for soft-margin SVM is:

\[
\min_{w, b, \xi} \quad \frac{1}{2} \|w\|^2 + C \sum_n \xi_n
\]

where:
- \( C \) **controls the trade-off between margin size and misclassification**.
  - **Larger \( C \)** → Penalizes misclassification more (**strict model**).
  - **Smaller \( C \)** → Allows more misclassification (**flexible model**).

##### Why is This a Problem?

- The **primal formulation still assumes a linear relationship** in the original feature space.
- There is **no way to incorporate nonlinear decision boundaries** unless we manually modify the features.

##### Next Step → Dual Formulation

- By switching to the **dual formulation**, we can express the optimization in terms of **dot products**, making **kernelization possible**.

---

## Converting Primal to Dual Formulation

To derive the **dual formulation**, we use **Lagrange multipliers**.

#### 2.1 Lagrangian Function

We introduce **Lagrange multipliers** \( \alpha_n \) for the constraints:

\[
\mathcal{L}(w, b, \xi, \alpha, \lambda) = \frac{1}{2} \|w\|^2 + C \sum_n \xi_n
\]

\[-\sum_n \alpha_n [ y_n (w^T x_n + b) - 1 + \xi_n ] 
\]

\[
-\sum_n \lambda_n \xi_n
\]

where:
- \( \alpha_n \geq 0 \) enforce the **classification constraints**.
- \( \lambda_n \geq 0 \) enforce the **non-negative slack constraints**.

#### 2.2 Solving for \( w \) and \( b \)

To move to the **dual problem**, we compute the **stationary conditions** by setting the derivatives of \( \mathcal{L} \) to zero.

##### Compute \( \frac{\partial \mathcal{L}}{\partial w} = 0 \):

\[
w = \sum_n \alpha_n y_n x_n
\]

This shows that **the optimal weight vector is a linear combination of the training points**.

##### Compute \( \frac{\partial \mathcal{L}}{\partial b} = 0 \):

\[
\sum_n \alpha_n y_n = 0
\]

This ensures that the **classification constraint is satisfied globally**.

##### Compute \( \frac{\partial \mathcal{L}}{\partial \xi_n} = 0 \) (Only for Soft-Margin)

\[
\alpha_n = C - \lambda_n
\]

Since \( \lambda_n \geq 0 \), we get the constraint:

\[
0 \leq \alpha_n \leq C
\]

---

## Dual Formulation of SVM

After substituting \( w = \sum_n \alpha_n y_n x_n \) into \( \mathcal{L} \), we obtain the **dual optimization problem**:

\[
\max_{\alpha} \quad \sum_n \alpha_n - \frac{1}{2} \sum_{m,n} y_m y_n \alpha_m \alpha_n (x_m^T x_n)
\]

where **\((x_m^T x_n) = \phi(x_m)^T \phi(x_n)\)**
subject to:

\[
0 \leq \alpha_n \leq C, \quad \sum_n \alpha_n y_n = 0
\]

where:
- \( \alpha_n \) are **Lagrange multipliers** representing the contribution of each data point.
- The **decision function** for a test point \( x \) is:

    \[
    h(x) = \text{sign} \left( \sum_n \alpha_n y_n (x_n^T x) + b \right)
    \]

    where **the \(x_n^Tx\) can be replaced by the kernel function \(k(x, x_n)\)**.

#### Key Observations:
- The **dual formulation depends only on dot products** \( x_m^T x_n \), allowing **kernelization**.
- **Only support vectors** (\( \alpha_n > 0 \)) influence the decision boundary.
- **Computationally advantageous** when \( d \) (features) is large but \( N \) (samples) is small.

---

## Kernelized Dual SVM

The **kernel trick** allows us to extend SVM to **nonlinear classification** by **replacing dot products \( x_m^T x_n \) with a kernel function \(k(x_m,  x_n)\)**:

\[
k(x_m, x_n) = \phi(x_m)^T \phi(x_n) = x_m^T x_n
\]

Thus, the **kernelized dual problem** is:

\[
\max_{\alpha} \quad \sum_n \alpha_n - \frac{1}{2} \sum_{m,n} y_m y_n \alpha_m \alpha_n k(x_m, x_n)
\]

subject to:

\[
0 \leq \alpha_n \leq C, \quad \sum_n \alpha_n y_n = 0
\]

The **decision function** becomes:

\[
h(x) = \text{sign} \left( \sum_n \alpha_n y_n k(x_n, x) + b \right)
\]

---

## Q1. Is *Dual* SVM the kernelized version of *Primal* SVM?

> Not exactly! The dual formulation of SVM is **NOT** inherently the kernelized version of the primal SVM, but rather, it **enables the use of the kernel trick**, which allows SVMs to handle **nonlinear decision boundaries**.

### 1. Relationship Between Primal, Dual SVM and Kernelized SVM

- The **dual formulation** is a mathematically equivalent reformulation of the **primal formulation** using **Lagrange multipliers**.
- The **primal SVM** explicitly solves for the weight vector \( w \), while the **dual SVM** solves for the Lagrange multipliers \( \alpha_n \), which define the importance of each training point.
- The **kernel trick** is only applicable in the **dual formulation**, but the **dual formulation itself does not automatically mean the SVM is kernelized**.

### 2. Primal SVM (Linear Decision Boundary)

The **primal formulation** of SVM explicitly solves for \( w \) and \( b \):

\[
\min_{w,b} \quad \frac{1}{2} ||w||^2 + C \sum_n \xi_n
\]

subject to:

\[
y_n (w^T x_n + b) \geq 1 - \xi_n, \quad \xi_n \geq 0, \quad \forall n
\]

##### **Key Limitation:**
- The decision function is **linear**, meaning it only works well if the data is **linearly separable (Hard-SVM)**.
- If data is **not linearly separable (Soft-SVM)**, a more flexible model (Dual SVM) is needed.

### 3. Dual SVM for Soft-SVM (Without Kernelization)

The **dual formulation** expresses the problem in terms of **dot products** between training points:

\[
\max_{\alpha} \quad \sum_n \alpha_n - \frac{1}{2} \sum_{m,n} y_m y_n \alpha_m \alpha_n (\phi(x_m)^T \phi(x_n))
\]

where **\(\phi(x_m)^T \phi(x_n) = x_m^T x_n\)**,
subject to:

\[
0 \leq \alpha_n \leq C, \quad \sum_n \alpha_n y_n = 0
\]

### 4. Kernel Trick: Turning Dual SVM into Kernelized SVM

In the **kernelized version of SVM**, we **replace the dot product \(\phi(x_m)^T \phi(x_n) = x_m^T x_n\) with a kernel function \( k(x_m, x_n) \)**:

\[
k(x_m, x_n) = \phi(x_m)^T \phi(x_n)
\]

This modifies the **dual formulation** to:

\[
\max_{\alpha} \quad \sum_n \alpha_n - \frac{1}{2} \sum_{m,n} y_m y_n \alpha_m \alpha_n k(x_m, x_n)
\]

subject to:

\[
0 \leq \alpha_n \leq C, \quad \sum_n \alpha_n y_n = 0
\]

##### **Why This Matters?**
- This formulation **depends only on the dot products** \( x_m^T x_n \), not on explicit feature representations.
- We can **replace the dot product** \( x_m^T x_n \) with a **kernel function** \( k(x_m, x_n) \), allowing SVMs to work in **higher-dimensional feature spaces** without explicitly computing those transformations.

#### Summary
| Feature                     | **Primal SVM** | **Dual SVM (Linear)** | **Dual SVM (Kernelized)** |
|-----------------------------|---------------|-----------------------|---------------------------|
| **Optimization Variables**  | \( w \)       | \( \alpha \)         | \( \alpha \)             |
| **Decision Function**       | \( f(x) = w^T x + b \) | \( f(x) = \sum_n \alpha_n y_n (x_n^T x) + b \) | \( f(x) = \sum_n \alpha_n y_n k(x_n, x) + b \) |
| **Linear Decision Boundary?** | ✅ Yes | ✅ Yes | ❌ No (Nonlinear) |
| **Uses Kernel Trick?**      | ❌ No | ❌ No | ✅ Yes |
| **Computational Cost**      | \( O(Nd) \) | \( O(N^2) \) | \( O(N^2) \) |

---

## Q2. Can *Primal* SVM handle nonlinear decision boundaries?

> No\! Primal SVM **cannot handle** nonlinear decision boundaries directly because it explicitly learns a weight vector \( w \) in the original feature space. Unlike the dual formulation, which can incorporate the **kernel trick** to implicitly transform data into higher-dimensional spaces, the **primal formulation** operates in the **original input space** and thus only finds **linear decision boundaries**.

The **primal formulation** still assumes a **linear relationship** because it explicitly solves for a weight vector *w* in the original feature space, leading to a decision function of the form:

$$
f(x) = w^T x + b
$$

Since *w* operates directly on the input features *x*, the decision boundary is always **a straight line (in 2D) or a hyperplane (in higher dimensions)**. There is **no mechanism to introduce nonlinear transformations** unless we manually modify the features.
