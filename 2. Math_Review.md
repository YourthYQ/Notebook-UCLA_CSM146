## Math Review

#### 1. $\ell_1$-Norm (Manhattan Norm or Taxicab Norm)

The $\ell_1$-norm measures the **sum of the absolute differences** between the components of two vectors. It is also called the **Manhattan distance** because it corresponds to how you would measure distance by moving along a grid (like the streets of Manhattan).

##### Formula:
For a vector $x = [x_1, x_2, \dots, x_D]$:
$$
\|x\|_1 = \sum_{d=1}^{D} |x_d|
$$

For the distance between two points $x = [x_1, x_2, \dots, x_D]$ and $y = [y_1, y_2, \dots, y_D]$:
$$
\|x - y\|_1 = \sum_{d=1}^{D} |x_d - y_d|
$$

##### Example:
For $x = [1, 2]$ and $y = [4, 6]$:
$$
\|x - y\|_1 = |1 - 4| + |2 - 6| = 3 + 4 = 7
$$

---

#### 2. $\ell_2$-Norm (Euclidean Norm)

The $\ell_2$-norm measures the **straight-line distance** between two points in Euclidean space. It calculates the **root of the sum of the squared differences** between vector components.

##### Formula:
For a vector $x = [x_1, x_2, \dots, x_D]$:
$$
\|x\|_2 = \sqrt{\sum_{d=1}^{D} (x_d)^2}
$$

For the distance between two points $x = [x_1, x_2, \dots, x_D]$ and $y = [y_1, y_2, \dots, y_D]$:
$$
\|x - y\|_2 = \sqrt{\sum_{d=1}^{D} (x_d - y_d)^2}
$$

##### Example:
For $x = [1, 2]$ and $y = [4, 6]$:
$$
\|x - y\|_2 = \sqrt{(1 - 4)^2 + (2 - 6)^2}
$$

Step-by-step:
- Calculate the squared differences:
  - $(1 - 4)^2 = (-3)^2 = 9$
  - $(2 - 6)^2 = (-4)^2 = 16$
- Add the squared differences:
  $9 + 16 = 25$
- Take the square root:
  $\sqrt{25} = 5$

Thus, the $\ell_2$-norm (Euclidean distance) between $x$ and $y$ is:
$$
\|x - y\|_2 = 5
$$

---

#### 3. General $\ell_p$-Norm

The $\ell_p$-norm generalizes the idea of distance by raising the differences to the power $p$, summing them, and taking the $\frac{1}{p}$-power of the result.

##### Formula:
For a vector $x = [x_1, x_2, \dots, x_D]$:
$$
\|x\|_p = \left( \sum_{d=1}^{D} |x_d|^p \right)^{\frac{1}{p}}
$$

For the distance between two points $x = [x_1, x_2, \dots, x_D]$ and $y = [y_1, y_2, \dots, y_D]$:
$$
\|x - y\|_p = \left( \sum_{d=1}^{D} |x_d - y_d|^p \right)^{\frac{1}{p}}
$$

---

#### 4. `arg`

The term **arg** in mathematics, specifically in expressions like **arg min** or **arg max**, stands for **argument**. It is used to indicate the input value (or values) of a function that achieves a specific result, such as the minimum or maximum of the function.

##### Definitions:
- **arg min**: Finds the input $x$ that minimizes a given function.
- **arg max**: Finds the input $x$ that maximizes a given function.

##### Example 1: `arg min`

Consider the function:
$$
f(x) = (x - 3)^2
$$

- To find **arg min $f(x)$**, we look for the $x$ that minimizes $f(x)$.
- The function $f(x)$ is minimized when $x = 3$, because $f(3) = 0$, which is the smallest value of $f(x)$.

So:
$$
\text{arg min } f(x) = 3
$$

##### Example 2: `arg max`

Consider the function:
$$
g(x) = -x^2 + 4
$$

- To find **arg max $g(x)$**, we look for the $x$ that maximizes $g(x)$.
- The function $g(x)$ is maximized when $x = 0$, because $g(0) = 4$, which is the largest value of $g(x)$.

So:
$$
\text{arg max } g(x) = 0
$$

>  In the Context of Nearest Neighbors
> In nearest neighbor classification, **arg min** is used to find the index $n$ of the training point $x_n$ that minimizes the distance $\|x - x_n\|_2^2$.
> Example:
If the query point is $x = (1, 2)$ and training points are:
> - $x_1 = (0, 0)$: $\|x - x_1\|_2^2 = 5$
> - $x_2 = (2, 2)$: $\|x - x_2\|_2^2 = 1$
> Then:
$$
\text{arg min}_{n \in \{1, 2\}} \|x - x_n\|_2^2 = 2
$$
> This means that $x_2$ is the nearest neighbor of $x$.