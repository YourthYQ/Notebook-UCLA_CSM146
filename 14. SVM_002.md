## Loss Functions for Classification

Loss functions are essential in machine learning, as they measure how well a model’s predictions align with actual outcomes. In **classification tasks**, particularly **binary classification**, two key loss functions are:

1. **0/1 Loss** – The most intuitive but impractical loss for direct optimization.
2. **Hinge Loss** – A surrogate loss function used in Support Vector Machines (SVMs).

---

### 1. `0/1 Loss`

The **0/1 loss function** directly measures whether a prediction is correct or incorrect. It is defined as:

\[
\ell_{0/1}(y, \hat{y}) =
\begin{cases}
0, & \text{if } y = \hat{y} \\
1, & \text{if } y \neq \hat{y}
\end{cases}
\]

where:
- \( y \) is the true label (\(+1\) or \(-1\)),
- \( \hat{y} \) is the predicted label.

##### Example
| Sample | True Label (\( y \)) | Prediction (\( \hat{y} \)) | 0/1 Loss |
|--------|-----------------|-----------------|---------|
| A      | +1             | +1             | 0       |
| B      | -1             | +1             | 1       |
| C      | -1             | -1             | 0       |
| D      | +1             | -1             | 1       |

**If the prediction is correct, the loss is 0; otherwise, it's 1.**

##### Problems with 0/1 Loss
- **Non-convex & discontinuous**: The function does not change smoothly, making it hard to optimize.
- **Not differentiable**: Gradient-based optimization methods (e.g., gradient descent) cannot be used.
- **Computationally expensive**: Finding the optimal classifier under 0/1 loss is an **NP-hard** problem.

---

### 2. `Hinge Loss`

The **hinge loss** function is a convex approximation of the 0/1 loss, making it **easier to optimize**. It is defined as:

\[
\ell_{\text{hinge}}(y, a(x)) =
\begin{cases}
0, & \text{if } ya(x) \geq 1 \\
1 - ya(x), & \text{otherwise}
\end{cases}
\]

where:
- \( y \): The **true label** of the data point, typically \( y \in \{-1,1\} \) for binary classification.

- \( a(x) \): The **raw classifier output**, defined as \(a(x) = \mathbf{w}^T \phi(x) + b\), where \( \mathbf{w} \) is the weight vector, \( \phi(x) \) is the feature representation of \( x \), and \( b \) is the bias.

- \( y a(x) \): The **margin**, which measures the confidence of the classification. It is the product of the true label \( y \) and the classifier output \( a(x) \).


##### Convenient Notation
\[
\ell_{\text{hinge}}(y, a(x)) = \max(0, 1 - ya(x))
\]

##### Example
| Sample | True Label (\( y \)) | Model Output (\( a(x) \)) | \( ya(x) \) | Hinge Loss \( \max(0, 1 - ya(x)) \) |
|--------|-----------------|-----------------|---------|----------------|
| A      | +1             | 2.5             | 2.5     | 0              |
| B      | -1             | 1.2             | -1.2    | 2.2            |
| C      | -1             | -0.8            | 0.8     | 0.2            |
| D      | +1             | 0.3             | 0.3     | 0.7            |

##### Intuition
- If \( y \) and \( a(x) \) have the **same sign** (\( ya(x) > 0 \)), the prediction is **correct**.
- If \( y \) and \( a(x) \) have **opposite signs** (\( ya(x) < 0 \)), the prediction is **incorrect**.

Further, the **magnitude** of \( ya(x) \) tells us about the confidence of the classification:

- \( ya(x) \geq 1 \) → **Correct** and confidently classified (no loss).
- \( 0 \leq ya(x) < 1 \) → Correct but **too close** to the decision boundary (small loss).
- \( ya(x) < 0 \) → **Incorrect** classification (high loss).

##### Why Use Hinge Loss?
- **Convex** → Easier to optimize.  
- **Differentiable** (except at \( ya(x) = 1 \)) → Can use gradient-based optimization.  
- **Encourages large margins** → Leads to better generalization.

---

### Comparison: `0/1 Loss` vs. `Hinge Loss`

| Feature                | 0/1 Loss         | Hinge Loss                 |
|------------------------|-----------------|----------------------------|
| **Definition**          | 1 if incorrect, 0 if correct | \( \max(0, 1 - ya(x)) \) |
| **Optimization**        | Hard (NP-hard)  | Easier (convex)            |
| **Gradient-based Methods?** | ❌ No | ✅ Yes |
| **Convexity**          | ❌ No | ✅ Yes |
| **Sensitivity to Outliers** | High | Lower |

---

## Primal Formulation of SVM

The **primal formulation** of SVM is an optimization problem where we aim to find a **hyperplane** that maximizes the margin. In the **soft-margin SVM**, we allow some misclassification using slack variables \( \xi_n \), whereas in the **hard-margin SVM**, all points must be correctly classified.


#### **Step 1: Decision Boundary**  

A hyperplane in \( d \)-dimensional space is given by:  

\[
w^T x + b = 0
\]

where:  
- \( w \) is the **weight vector** (determines orientation).  
- \( b \) is the **bias term** (determines position).  

For a correctly classified point \( (x_n, y_n) \), we require:  

\[
y_n (w^T x_n + b) \geq 1
\]

#### **Step 2: Optimization Objective**  

The goal is to **maximize the margin**, which is equivalent to minimizing \( \|w\|^2_2 \). This leads to the **hard-margin SVM optimization problem**:  

\[
\min_{w, b} \quad \frac{1}{2} \|w\|^2_2
\]

subject to:  

\[
y_n (w^T x_n + b) \geq 1, \quad \forall n
\]

**Problem:** This works **only when data is perfectly separable**.  
**Solution:** Introduce **soft margins** with **slack variables** \( \xi_n \).  

#### **Step 3: Soft-Margin SVM (Allowing Misclassifications)**  

In real-world data, perfect separability is rare. We introduce **slack variables** \( \xi_n \) to allow some misclassification:  

\[
y_n (w^T x_n + b) \geq 1 - \xi_n, \quad \xi_n \geq 0, \quad \forall n
\]

where:  
- \( \xi_n = 0 \) means the point is correctly classified and outside the margin.  
- \( 0 < \xi_n < 1 \) means the point is correctly classified but within the margin.  
- \( \xi_n > 1 \) means the point is misclassified.  

#### **Step 4: Final Optimization Problem**  

We balance **margin maximization** and **classification error**:  

\[
\min_{w, b, \xi} \quad \frac{1}{2} \|w\|^2 + C \sum_{n} \xi_n
\]

subject to:  

\[
y_n (w^T x_n + b) \geq 1 - \xi_n, \quad \xi_n \geq 0, \quad \forall n
\]

where:  
- **\( C \) is a hyperparameter** that controls the trade-off:  

  - **Large \( C \)** → Penalizes misclassification more (**low margin**).  
  - **Small \( C \)** → Allows more misclassification (**high margin**).  

#### Relationship Between Hinge Loss and Primal Formulation  

The **hinge loss function** naturally appears in the primal SVM formulation. 
Rewriting the optimization problem:  

\[
\min_{w, b} \sum_{n} \max(0, 1 - y_n (w^T x_n + b)) + \frac{\lambda}{2} \|w\|^2
\]

where:  
- The **first term** represents **hinge loss**.  
- The **second term** is a **regularization term** to prevent overfitting.  
- \( \lambda \) controls regularization \( (\lambda = \frac{1}{C}) \).  

This shows that **SVM training directly optimizes hinge loss while ensuring a large margin**.  
