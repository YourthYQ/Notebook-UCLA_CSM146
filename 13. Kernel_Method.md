## Kernel Trick

The gradient descent update, or stochastic gradient update above becomes **computationally expensive** when the features \( \phi(x) \) is high-dimensional. 
For example, consider the direct extension of the feature map in equation to high-dimensional input \( x \). Suppose **\( x \in \mathbb{R}^D \), where \(D = 3\)**, and let \( \phi(x) \) be the vector that contains all the monomials of \( x \) with degree \( \leq 3 \):

\[
x = 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix} \Rightarrow \phi(x) =
\begin{bmatrix}
1 \\
x_1 \\
x_2 \\
\vdots \\
x_1^2 \\
x_1 x_2 \\
x_1 x_3 \\
\vdots \\
x_2 x_1 \\
\vdots \\
x_1^3 \\
x_1^2 x_2 \\
\vdots
\end{bmatrix}
\]

The dimension of the features \( \phi(x) \) is on the **order of \( D^3 \)**. This is a **prohibitively long vector** for computational purposes, namely the inner product **\( \theta^T \phi(x) \)** from the gradient descent -- When \( D = 1000 \), each update requires at least computing and storing a \( 1000^3 = 10^9 \) dimensional vector, which is \( 10^6 \) times slower than the update rule for ordinary least squares updates.

#### Kernel Trick: Avoiding Direct Computation of \( \phi(x) \)

> The **kernel trick** is a powerful mathematical technique that allows us to **implicitly** map data into a higher-dimensional space **without explicitly computing the transformation**. This makes it possible to apply **linear models** (such as Support Vector Machines or Linear Regression) to problems that are inherently **nonlinear**.


Instead of explicitly computing the high-dimensional transformation \( \phi(x) \), we can use a **kernel function** \( k(x_i, x_j) \) that computes:

\[
k(x_i, x_j) = \phi(x_i)^T \phi(x_j)
\]

where:

- \( x_i, x_j \) are original input vectors.
- \( \phi(x) \) is a function that maps \( x \) into a **higher-dimensional feature space**.
- \( k(x_i, x_j) \) is the **kernel function** that **computes the inner product in the transformed space without computing \( \phi(x) \) explicitly**.

Using **kernels**, the computational cost **remains at \( O(D) \) instead of \( O(M) \)**, making kernelized models more efficient.


#### Example: Quadratic Polynomial Expansion

Consider a simple **2D input vector** \( x = (x_1, x_2) \). A **quadratic polynomial transformation** maps it to a **higher-dimensional feature space** (from \(D = 2\) to \(M = 3\)):

\[
x = 
\begin{bmatrix}
x_1 \\
x_2 \\
\end{bmatrix} \Rightarrow
\phi(x) =
\begin{bmatrix}
x_1^2 \\
\sqrt{2} x_1 x_2 \\
x_2^2
\end{bmatrix}
\]

For two data points \( x_m \) and \( x_n \), the inner product in the transformed space is:

\[
\begin{aligned}
\phi(x_m)^T \phi(x_n) &= x_{m1}^2 x_{n1}^2 + 2x_{m1}x_{m2}x_{n1}x_{n2} + x_{m2}^2 x_{n2}^2 \\
&= (x_{m1}x_{n1} + x_{m2}x_{n2})^2 \\
&= (x_m^T x_n)^2
\end{aligned}
\]

which means we can **compute this inner product without explicitly transforming the data** by using the function:

\[
k(x_m, x_n) = (x_m^T x_n)^2
\]

This is a **polynomial kernel** of degree \( D = 2 \).

---

## Define a Kernel Function


> A function \( k(x_m, x_n) \) is a **valid kernel** if there exists a feature mapping \( \phi(x) \) such that:
>
> \[
> k(x_m, x_n) = k(x_n, x_m) \quad \text{(Symmetry)}
> \]
>
> \[
> k(x_m, x_n) = \phi(x_m)^T \phi(x_n) \quad \text{for some function } \phi(\cdot)
> \]
> 
> To determine if there exists such \(\phi(x)\), one necessary condition is that the **Gram matrix \( K \) must be positive semi-definite (PSD)**, namely, **all its eigenvalues are non-negative**.

#### 1. Gram Matrix \(K\)

The **Gram matrix** (also called the **kernel matrix**) is a symmetric matrix where each entry is given by a kernel function applied to a pair of data points. Mathematically, for a dataset with \( N \) points \( x_1, x_2, \dots, x_N \), the Gram matrix is defined as:

\[
\begin{aligned}
K &= \begin{bmatrix}
k(x_1, x_1) & k(x_1, x_2) & \dots & k(x_1, x_N) \\
k(x_2, x_1) & k(x_2, x_2) & \dots & k(x_2, x_N) \\
\vdots & \vdots & \vdots & \vdots \\
k(x_N, x_1) & k(x_N, x_2) & \dots & k(x_N, x_N)
\end{bmatrix} \\
&= \begin{bmatrix}
x_1^T x_1 & x_1^T x_2 & \dots & x_1^T x_N \\
x_2^T x_1 & x_2^T x_2 & \dots & x_2^T x_N \\
\vdots & \vdots & \vdots & \vdots \\
x_N^T x_1 & x_N^T x_2 & \dots & x_N^T x_N
\end{bmatrix}
\end{aligned}
\]

where \( k(x_i, x_j) = x_i^T x_j \) is a kernel function.

#### 2. Positive Semi-Definite (PSD) Matrix

A matrix \( K \) is **positive semi-definite (PSD)** if, for any vector \( v \), the quadratic form:

\[
v^T K v \geq 0
\]

for all possible nonzero \( v \).

**Equivalent to:**

A matrix \( K \) is PSD if:

1. **All eigenvalues of** \( K \) **are non-negative**.
2. **For every vector** \( v \), \( v^T K v \geq 0 \).
3. **It is symmetric:** \( K^T = K \) (which the Gram matrix always is).

**Simplifies to:**

To check if \( K \) is positive semi-definite, we compute its **eigenvalues**. If all eigenvalues are **non-negative**, the matrix is PSD.


**The function** \( k(x_m, x_n) = \|x_m - x_n\|^2 \) **does not satisfy this condition!**

---

#### Example: Is \( k(x_m, x_n) = \|x_m - x_n\|^2 \) **a Kernel ?**

To determine whether a function is a **valid kernel**, it must satisfy **Mercer's theorem**, which requires that the corresponding **Gram matrix** is **positive semi-definite (PSD)** for any set of input points.

Let's analyze why \( k(x_m, x_n) = \|x_m - x_n\|^2 \) fails to be a valid kernel.

##### **Step 1: Expand the Expression**

The squared Euclidean distance can be rewritten as:

\[
\|x_m - x_n\|^2 = (x_m - x_n)^T (x_m - x_n)
\]

Expanding this:

\[
= x_m^T x_m - 2x_m^T x_n + x_n^T x_n
\]

Thus, the function:

\[
k(x_m, x_n) = \|x_m - x_n\|^2 = x_m^T x_m + x_n^T x_n - 2x_m^T x_n
\]

##### **Step 2: Construct the Gram Matrix**

For a dataset with three points \( x_1, x_2, x_3 \), the Gram matrix \( K \) is:

\[
K =
\begin{bmatrix}
k(x_1, x_1) & k(x_1, x_2) & k(x_1, x_3) \\
k(x_2, x_1) & k(x_2, x_2) & k(x_2, x_3) \\
k(x_3, x_1) & k(x_3, x_2) & k(x_3, x_3)
\end{bmatrix}
\]

we get:

\[
K =
\begin{bmatrix}
0 & \|x_1 - x_2\|^2 & \|x_1 - x_3\|^2 \\
\|x_2 - x_1\|^2 & 0 & \|x_2 - x_3\|^2 \\
\|x_3 - x_1\|^2 & \|x_3 - x_2\|^2 & 0
\end{bmatrix}
\]

##### **Step 3: Check Positive Semi-Definiteness**

For \( K \) to be a valid kernel matrix, it must be **positive semi-definite (PSD)**, meaning that for any vector \( v \):

\[
v^T K v \geq 0
\]

for all \( v \neq 0 \).

However, in many cases, the **Gram matrix** constructed from squared Euclidean distances has **negative eigenvalues**, meaning it is **not PSD**.

Thus, \( k(x_m, x_n) = \|x_m - x_n\|^2 \) **does not define a valid kernel function** because it can produce a **Gram matrix that is not positive semi-definite**.

---

## Common Kernel Functions

Two of the most widely used kernel functions are the **polynomial kernel** and the **Gaussian Radial Basis Function (RBF) kernel**.

### 1. Polynomial Kernel

The **polynomial kernel** is defined as:

\[
k(x_m, x_n) = (x_m^T x_n + c)^d
\]

where:

- \( x_m, x_n \in \mathbb{R}^D \) are input vectors.
- \( c \geq 0 \) is a hyperparameter that controls flexibility.
- \( d \) is the **degree** of the polynomial (a positive integer).

##### Key Properties:

- Captures **nonlinear relationships** by expanding the input space into polynomial features.
- Higher-degree polynomials allow more complex decision boundaries.
- **Hyperparameters to tune**: \( c \) (constant term) and \( d \) (polynomial degree).

##### Example:

For a **quadratic kernel** (\( d = 2, c = 1 \)):

\[
k(x_m, x_n) = (x_m^T x_n + 1)^2
\]

Expanding this:

\[
(x_m^T x_n + 1)^2 = x_m^T x_n x_m^T x_n + 2 x_m^T x_n + 1
\]

This is equivalent to **mapping data into a higher-dimensional space with quadratic features**.


### 2. Gaussian (RBF) Kernel

The **Gaussian Radial Basis Function (RBF) kernel** is defined as:

\[
k(x_m, x_n) = e^{-\frac{\| x_m - x_n \|^2}{2\sigma^2}}
\]

where:

- \( \| x_m - x_n \|^2 \) is the squared Euclidean distance between two points.
- \( \sigma^2 \) (variance) controls the spread of the kernel.

##### Key Properties:

- Maps data into an **infinite-dimensional feature space**, making it highly flexible.
- Only depends on the **difference** between two inputs.
- The smaller \( \| x_m - x_n \| \), the larger \( k(x_m, x_n) \), meaning **closer points have higher similarity**.
- **Hyperparameter to tune**: \( \sigma^2 \), which controls the width of the kernel.

##### Why Use Gaussian Kernels?

- Effective for **highly nonlinear** problems.
- Works well when we **do not know the explicit feature transformation**.

### 3. Summary

- **Polynomial kernel**: Expands feature space explicitly with polynomials.
- **Gaussian (RBF) kernel**: Implicitly maps data into an infinite-dimensional space.
- Both **allow nonlinear classification using linear models like SVM**.
