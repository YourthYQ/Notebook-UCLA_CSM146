## Before Note: The Netflix Prize

The Netflix Prize was a competition held by Netflix in 2006 to improve their movie recommendation system. The goal was to **predict user ratings for movies** more accurately than Netflix’s existing algorithm.

#### 0.1 Problem Statement

Netflix's recommendation system, called **Cinematch**, used collaborative filtering to predict movie ratings based on:

- Other users' ratings of the same movie.
- A user’s ratings of other movies.

The challenge was to improve Netflix’s system by **at least 10%**, with a **$1,000,000 prize** for the best model.

#### 0.2 How Did the Winners Use Ensemble Methods?

> **Ensemble learning** is a powerful technique in machine learning where multiple models (classifiers) are combined to improve overall performance. Instead of relying on a single model, ensemble methods aggregate the predictions of multiple models to produce a stronger, more accurate final prediction.

After years of experimentation, the winning team, called **BellKor's Pragmatic Chaos**, achieved a **10.06% improvement** by using an ensemble of **over 800 different models**!

Instead of relying on **one** best model, they combined many different models, including:

1. Matrix Factorization Models (e.g., Singular Value Decomposition - SVD)
2. Restricted Boltzmann Machines (RBMs)
3. Neighborhood-based Methods (e.g., K-Nearest Neighbors for recommendations)
4. Regression Models
5. Decision Trees and Gradient Boosting Models
6. Neural Networks

#### 0.3 How Did They Combine 800+ Models?

They used **weighted averaging** and **blending techniques**:

- Each model was trained on Netflix’s dataset separately.
- The final prediction was a **weighted combination of all models**.
- Models that performed better on validation data were given **higher weights**.

#### 0.4 Why Was Ensemble Learning So Effective?

1. **Captured Different Patterns**: Each model specialized in different aspects of user preferences.
2. **Reduced Overfitting**: Since models were trained on **diverse** subsets, they generalized better.
3. **More Robust Predictions**: Even if one model was weak, others compensated for its errors.

#### 0.5 Lessons from the Netflix Prize

The Netflix Prize demonstrated:

- **More models ≠ better accuracy**. Instead, combining diverse, complementary models is key.
- **Averaging multiple models reduces error and variance**.
- **Ensemble methods are crucial in real-world AI applications** like recommendation systems, fraud detection, and medical diagnosis.

---

## Ensemble (集成) Methods

Ensemble learning is a powerful technique in machine learning where multiple models (classifiers) are combined to improve overall performance. Instead of relying on a single model, ensemble methods aggregate the predictions of multiple models to produce a stronger, more accurate final prediction.

#### 1.1 Why Do Ensemble Methods Work?

Ensemble methods leverage diversity among individual models. The key idea is:

- Different models make different types of mistakes.
- If models have **low correlation**, their mistakes can cancel out, leading to an overall better prediction.

#### 1.2 Types of Ensemble Methods

There are two primary ways to implement ensemble learning:

1. **Multiple classifiers** trained on the **same dataset** (e.g., different algorithms like Decision Trees, Logistic Regression, and SVMs combined).
2. **The same classifier** trained on **multiple variations of the dataset** (e.g., using techniques like **Bagging** and **Boosting**).

#### 1.3. Multiple Classifiers on the Same Dataset

A common approach is to train different classifiers (e.g., decision trees, logistic regression, neural networks) on the same dataset and combine their predictions using:

1. **Majority Vote**: The final prediction is the most frequent class predicted by individual models.
   \[
   h = \text{sign}(h_1 + h_2 + \dots + h_L)
   \]
   where each \( h_i \) outputs either +1 or -1.

2. **Weighted Majority Vote**: Some models are given more weight based on their performance.
   \[
   h = \text{sign}(w_1 h_1 + w_2 h_2 + \dots + w_L h_L)
   \]
   The weights are learned using validation data.

3. **Regression Models**: Instead of voting, use **mean**, **weighted mean**, or **median** of predictions.

---

## Bagging (Bootstrap Aggregation)

Bagging is a technique that involves **training multiple instances of the SAME classifier** on **different random subsets of the training data**. The key idea is:

- By training multiple models on different subsets of data, each model captures different patterns.
- The final prediction is made by **combining the predictions** of all instances of model, typically using a  
  **majority vote (for classification)** or **averaging (for regression)**.

#### 3.1 How Does Bagging Work?

The process of bagging follows these steps:

1. Create Multiple Datasets (Bootstrap Resampling)
   - Given a dataset \( D \) with \( N \) instances, generate B bootstrap datasets \( \tilde{D}_1, \tilde{D}_2, ..., \tilde{D}_B \).
   - Each \( \tilde{D}_b \) is created by **randomly sampling** \( N \) examples **with replacement** from \( D \).
   - Some examples may appear **multiple times**, while others may be **excluded**.

2. Train a Classifier on Each Dataset
   - Train **B classifiers** \( h_1, h_2, ..., h_B \), each on a different \( \tilde{D}_b \).
   - Typically, the classifier used is a **Decision Tree**, but any model can be used.

3. Combine Predictions
   - **For classification**: Use **majority voting**.
   - **For regression**: Use **averaging** of predictions.

##### Example of Bagging: Random Forest

- **Random Forest** is a famous algorithm based on bagging.
- It trains multiple **decision trees** on bootstrapped datasets.
- Instead of using the full feature set, each tree is trained on a **random subset of features**.
- The final prediction is made by aggregating predictions from all trees.

#### 3.1 Why Does Bagging Work?

- Reduces Overfitting: A single model may overfit the training data, but averaging multiple models reduces this risk.
- Decreases Variance: Since each model is trained on different data, their individual biases cancel out.
- Improves Generalization: The final ensemble model performs better on unseen test data.

#### 3.2 Cross-Validation v.s. Bagging

- **Cross-validation** is a method used to **evaluate** a model by splitting the dataset into multiple parts,  
training the model on some parts, and testing it on the rest. It helps check how well the model will  
perform on new data and prevents **overfitting**.

- **Bagging** (Bootstrap Aggregation) is a technique to **improve** model performance by training multiple  
models on different random subsets of data and then combining their predictions. It reduces  
**variance** and makes the model more stable.

---

## Boosting

