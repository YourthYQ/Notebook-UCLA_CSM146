## Confusion Matrix

A confusion matrix is a table that summarizes a classification model's performance by showing the counts of correct and incorrect predictions.

#### 1.1 Structure of a Confusion Matrix

| Predicted Class | Actual Class: Positive (1) | Actual Class: Negative (0) |
|----------------|---------------------------|---------------------------|
| **Positive (1)** | True Positive (TP) | False Positive (FP) |
| **Negative (0)** | False Negative (FN) | True Negative (TN) |

#### 1.2 Explanation of Terms

- **True Positive (TP):** The model correctly predicts a positive instance.
- **False Positive (FP) (Type I Error):** The model incorrectly predicts a negative instance as positive.
- **False Negative (FN) (Type II Error):** The model incorrectly predicts a positive instance as negative.
- **True Negative (TN):** The model correctly predicts a negative instance.

##### Example: Spam Email Detection

- **TP:** The model correctly classifies spam emails as spam.
- **FP:** The model incorrectly classifies a non-spam email as spam.
- **FN:** The model incorrectly classifies a spam email as non-spam.
- **TN:** The model correctly classifies non-spam emails as non-spam.

---

## Model Evaluation Metrics

From the confusion matrix, we derive several useful evaluation metrics.

#### 2.1 Accuracy

Measures **overall correctness** of the model:

\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]

- **Good for balanced datasets** (equal number of positives and negatives).
- **Not reliable for imbalanced datasets** (e.g., if 99% of the data is negative, a model predicting everything as negative would have 99% accuracy but be useless).

#### 2.2 Precision (Positive Predictive Value - PPV)

Measures **how many of the predicted positives are actually correct**:

\[
\text{Precision} = \frac{TP}{TP + FP}
\]

- **High precision** means **fewer false positives**.
- **Important when false positives are costly** (e.g., detecting cancer—misdiagnosing a healthy person as sick is bad).

#### 2.3 Recall (Sensitivity or True Positive Rate - TPR)

Measures **how many actual positives were correctly identified**:

\[
\text{Recall} = \frac{TP}{TP + FN}
\]

- **High recall** means **fewer false negatives**.
- **Important when missing positives is costly** (e.g., detecting fraud—missing fraudulent transactions is bad).

#### 2.4 F1-Score

The **harmonic mean** of precision and recall, balancing both:

\[
F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\]

- **Useful when precision and recall need to be balanced.**
- **Especially good for imbalanced datasets.**

#### Summary of Metrics

| **Metric** | **Best When...** | **Example** |
|-----------|-----------------|------------|
| **Accuracy** | Classes are balanced | General classification problems |
| **Precision** | False positives are costly | Medical diagnosis (avoiding false alarms) |
| **Recall** | False negatives are costly | Fraud detection (catching all fraud cases) |
| **F1-Score** | Need a balance of precision & recall | NLP classification tasks |

---

## ROC and Precision-Recall Curves

For classification models, we often care about **confidence scores**, not just the final predicted label.  
This is where **ROC curves and Precision-Recall curves** help.

#### 3.1 ROC Curve (Receiver Operating Characteristic)

- Plots **True Positive Rate (Recall) vs. False Positive Rate** at different classification thresholds.
- **False Positive Rate (FPR):**

  \[
  FPR = \frac{FP}{FP + TN}
  \]

- The **closer to the upper-left corner**, the better the model.
- The **diagonal line** represents a random guess.

#### 3.2 AUC-ROC (Area Under the Curve)

- **AUC (Area Under ROC Curve)** measures the model's ability to distinguish between classes.
- Values range from **0 to 1**:
  - **1.0**: Perfect model.
  - **0.5**: Random guessing.
  - **< 0.5**: Worse than random guessing.

#### 3.3 Precision-Recall (PR) Curve

- Plots **Precision vs. Recall** at different thresholds.
- More useful when dealing with **imbalanced datasets** (e.g., fraud detection, rare disease classification).
- A **high Precision-Recall AUC** means the model effectively finds positives while keeping false positives low.
