## Feature Transformation

Recall that in our discussion about linear regression, we considered the problem of predicting **the price of a house (denoted by \( y \))** from **the living area of the house (denoted by \( x \))**, and we fit a linear function of \( x \) to the training data. What if the price \( y \) can be more accurately represented as a _non-linear_ function of \( x \)? In this case, we need a more expressive family of models than linear models.

We start by considering fitting cubic functions \( y = \theta_3 x^3 + \theta_2 x^2 + \theta_1 x + \theta_0 \). It turns out that we can view the cubic function as a linear function over a different set of feature variables (defined below). Concretely, let the function \( \phi : \mathbb{R} \to \mathbb{R}^4 \) be defined as

\[
\phi(x) =
\begin{bmatrix}
1 \\
x \\
x^2 \\
x^3
\end{bmatrix}
\in \mathbb{R}^4.
\]

Let \( \theta \in \mathbb{R}^4 \) be the vector containing \( \theta_0, \theta_1, \theta_2, \theta_3 \) as entries. Then we can rewrite the cubic function in \( x \) as:

\[
\theta_3 x^3 + \theta_2 x^2 + \theta_1 x + \theta_0 = \theta^T \phi(x).
\]

Thus, a cubic function of the variable \( x \) can be viewed as a linear function over the variables \( \phi(x) \). 

* To distinguish between these two sets of variables, in the context of kernel methods, we will call the **_original_ input value** the input **attributes** of a problem (in this case, \( x \), the living area). 
* When the original input is mapped to some new set of quantities \( \phi(x) \), we will call those **new quantities** the **features** variables. (Unfortunately, different authors use different terms to describe these two things in different contexts.) 
* We will call **\( \phi \)** a **feature map**, which maps the attributes to the features.

---

## Key Differences Before and After Transformation

> **Q1: Is the *input* the same thing before and after the transformation?**
> **A:** No, the **input is not the same** before and after the transformation.

Before the transformation, the input data points are in their **original feature space** (e.g., scalar values in \( \mathbb{R} \) if it's a 1D input, or vectors in \( \mathbb{R}^D \) if it's a multi-dimensional input).

After applying the transformation \( \phi(x) \), the input is mapped into a **new feature space**, which may have a higher dimension or even an infinite dimension (e.g., in the case of certain kernel functions).

| Aspect | Before Transformation \( x \) | After Transformation \( \phi(x) \) |
|--------|--------------------------------|--------------------------------|
| **Feature Space** | Original space \( \mathbb{R}^D \) | Transformed space \( \mathbb{R}^M \), where \( M \geq D \) |
| **Data Representation** | Raw input features | New features generated by \( \phi(x) \) |
| **Dimensionality** | Lower | Usually higher |

#### Example Comparison

##### Before Transformation (Original Inputs)

\[
(x_1, y_1) = (1,0), \quad (x_2, y_2) = (2,1), \quad (x_3, y_3) = (3,1)
\]

##### After Transformation (Mapped Inputs)

Using \( \phi(x) = [1, x, x^2] \):

\[
(\phi(x_1), y_1) =
\begin{pmatrix}
\begin{bmatrix}
1 \\
1 \\
1
\end{bmatrix},
0
\end{pmatrix},
\quad
(\phi(x_2), y_2) =
\begin{pmatrix}
\begin{bmatrix}
1 \\
2 \\
4
\end{bmatrix},
1
\end{pmatrix},
\quad
(\phi(x_3), y_3) =
\begin{pmatrix}
\begin{bmatrix}
1 \\
3 \\
9
\end{bmatrix},
1
\end{pmatrix}.
\]

Here, each **scalar input \( x \)** is transformed into a **3D vector** \( \phi(x) \), meaning that the input space has changed.

<br>

> **Q2: Is the *hypothesis function \( h_{\theta}(x) \)* the same thing before and after the transformation?**
> **A:** No.
> ps. _The function \( h_{\theta}(x) \) represents the modelâ€™s hypothesis (or decision function) used for prediction._

In the original feature space (before transformation), a linear model is typically represented as \(h_{\theta}(x) = \theta_0 + \theta_1 x\) for some parameter vector \( \theta = (\theta_0, \theta_1) \).
For multiple features, it generalizes to:

\[
h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_D x_D
\]

where \( x \) is a **D-dimensional** input.


After applying a feature transformation \( \phi(x) \), we now operate in a higher-dimensional space. The model remains **linear in the new features**, but the input is now \( \phi(x) \):

\[
h_{\theta}(x) = \theta_0 + \theta_1 \phi_1(x) + \theta_2 \phi_2(x) + \cdots + \theta_M \phi_M(x)
\]

This means that while the **model is still a linear function in terms of the new features \( \phi(x) \)**, it is **nonlinear with respect to the original input \( x \)**.

| Aspect | Before Transformation \( h_{\theta}(x) \) | After Transformation \( h_{\theta}(\phi(x)) \) |
|--------|--------------------------------|----------------------------------|
| **Feature Space** | Original input \( x \) | Transformed features \( \phi(x) \) |
| **Function Form** | Linear in \( x \) | Linear in \( \phi(x) \), but nonlinear in \( x \) |
| **Example** | \( h_{\theta}(x) = \theta_0 + \theta_1 x \) | \( h_{\theta}(x) = \theta_0 + \theta_1 x + \theta_2 x^2 \) |

#### Example: Quadratic Transformation

Suppose we use the quadratic transformation:

\[
\phi(x) =
\begin{bmatrix}
1 \\
x \\
x^2
\end{bmatrix}
\]

##### Before Transformation
  
  \[
  h_{\theta}(x) = \theta_0 + \theta_1 x
  \]

  (a simple linear function in \( x \)).

##### After Transformation
  
  \[
  h_{\theta}(x) = \theta_0 + \theta_1 x + \theta_2 x^2
  \]

  (now a **quadratic function** in terms of \( x \), though still linear in \( \phi(x) \)).

This transformation enables linear models to **capture non-linear relationships** in the data, which is the core idea behind **kernel methods and SVMs**.

---

## Gradient Descent & \(\text{Big } O\)

The process of **gradient descent** and the **computational complexity** (*O*-notation) change when we apply a transformation \( \phi(x) \), particularly when moving from a lower-dimensional to a higher-dimensional feature space.


#### 1. Before Transformation

Before applying a transformation, we typically deal with a **linear hypothesis function**:

\[
h_{\theta}(x) = \theta^T x
\]

Given a loss function (e.g., Mean Squared Error for regression or Logistic Loss for classification), the gradient descent update rule for **one training example** is:

\[
\theta := \theta - \alpha \nabla_{\theta} J(\theta)
\]

where \( J(\theta) \) is the cost function, and its gradient is:

\[
\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^{N} (h_{\theta}(x_i) - y_i)x_i
\]

##### Computational Complexity

- Each gradient update involves **computing the inner product \( \theta^T x_i \)**, which takes **\( O(D) \)** time (for **\( D \)-dimensional input**).
- If we perform **batch gradient descent**, updating the parameters over all \( N \) training examples requires **\( O(ND) \)** time per iteration.
- **Stochastic Gradient Descent (SGD)** performs updates in **\( O(D) \)** per training example.

#### 2. After Transformation

After applying a transformation \( \phi(x) \), the model now operates in a **higher-dimensional feature space** \( M \):

\[
h_{\theta}(\phi(x)) = \theta^T \phi(x)
\]

The gradient update now follows:

\[
\theta := \theta - \alpha \nabla_{\theta} J(\theta)
\]

where:

\[
\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^{N} (h_{\theta}(\phi(x_i)) - y_i) \phi(x_i)
\]

##### Computational Complexity

- If \( \phi(x) \) **increases the feature dimension** from \( D \) to \( M \), then **computing the inner product \( \theta^T \phi(x) \)** now **takes \( O(M) \) instead of \( O(D) \)**, where \(M \gg D\) (much greater than).
- In **batch gradient descent**, updating the parameters requires **\( O(NM) \)** per iteration.
- **SGD** takes **\( O(M) \)** per update.

Since **\( M > D \)** in most cases, the **computational cost increases** after transformation.

#### 3. \(\text{Big } O\) Comparison

| Step | Before Transformation \( O(D) \) | After Transformation \( O(M) \) |
|------|--------------------------------|--------------------------------|
| Hypothesis Evaluation \( h_{\theta}(x) \) | \( O(D) \) | \( O(M) \) |
| Gradient Computation \( \nabla_{\theta} J(\theta) \) | \( O(ND) \) | \( O(NM) \) |
| SGD Per Iteration | \( O(D) \) | \( O(M) \) |
| Batch Gradient Descent Per Iteration | \( O(ND) \) | \( O(NM) \) |

---

## Kernel Trick

The gradient descent update, or stochastic gradient update above becomes **computationally expensive** when the features \( \phi(x) \) is high-dimensional. 
For example, consider the direct extension of the feature map in equation to high-dimensional input \( x \). Suppose **\( x \in \mathbb{R}^D \), where \(D = 3\)**, and let \( \phi(x) \) be the vector that contains all the monomials of \( x \) with degree \( \leq 3 \):

\[
x = 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix} \Rightarrow \phi(x) =
\begin{bmatrix}
1 \\
x_1 \\
x_2 \\
\vdots \\
x_1^2 \\
x_1 x_2 \\
x_1 x_3 \\
\vdots \\
x_2 x_1 \\
\vdots \\
x_1^3 \\
x_1^2 x_2 \\
\vdots
\end{bmatrix}
\]

The dimension of the features \( \phi(x) \) is on the **order of \( D^3 \)**. This is a **prohibitively long vector** for computational purposes, namely the inner product **\( \theta^T \phi(x) \)** from the gradient descent -- When \( D = 1000 \), each update requires at least computing and storing a \( 1000^3 = 10^9 \) dimensional vector, which is \( 10^6 \) times slower than the update rule for ordinary least squares updates.

#### Kernel Trick: Avoiding Direct Computation of \( \phi(x) \)

Instead of explicitly computing the high-dimensional transformation \( \phi(x) \), we can use a **kernel function** \( k(x_i, x_j) \) that computes:

\[
k(x_i, x_j) = \phi(x_i)^T \phi(x_j)
\]

Using **kernels**, the computational cost **remains at \( O(D) \) instead of \( O(M) \)**, making kernelized models more efficient.
