# Regularization

Regularization is a technique used to **prevent overfitting** by reducing the complexity of models. It does this by **adding a penalty term to the loss function**, which controls the size of the model's parameters (weights).

### 1. How Does Regularization Work?

Regularization modifies the **loss function** by adding a penalty term that discourages large coefficients (weights).

\[
J(w) = \text{Original Loss} + \lambda \times \text{Regularization Term}
\]

Where:

- \(\lambda\) is the **regularization strength** (a hyperparameter).
- A **higher** \(\lambda\) **shrinks weights more**, making the model simpler.

### 2. Types of Regularization

##### (A) L2 Regularization (Ridge Regression)

L2 regularization **adds a penalty on the sum of squared weights**:

\[
J(w) = \sum (y_n - w^T x_n)^2 + \lambda \sum w_d^2
\]

- **Effect**: Shrinks all weights but does not force them to be exactly zero.
- **Prevents large weight values**, leading to a smoother and more stable model.
- **Common in linear regression (Ridge Regression).**

##### (B) L1 Regularization (Lasso Regression)

L1 regularization **adds a penalty on the sum of absolute weight values**:

\[
J(w) = \sum (y_n - w^T x_n)^2 + \lambda \sum |w_d|
\]

- **Effect**: Shrinks some weights completely to **zero**.
- **Good for feature selection**, as it automatically removes less important features.
- **Used in Lasso Regression.**

##### (C) Elastic Net Regularization

A combination of **L1 and L2 regularization**:

\[
J(w) = \sum (y_n - w^T x_n)^2 + \lambda_1 \sum |w_d| + \lambda_2 \sum w_d^2
\]

- **Effect**: Provides both **feature selection (L1)** and **weight shrinkage (L2)**.
- **Used when features are correlated.**

### 3. Relationship

Remember that \(\lambda \) have to be positive, namely **\(\lambda \geq 0 \)**.

| **Regularization Strength (\(\lambda\))** | **Model Complexity** | **Bias** | **Variance** | **Overfitting?** |
|--------------------------------------|-----------------|------|----------|--------------|
| \(\lambda = 0\) (No Regularization)  | High            | Low  | High     | Yes (Overfitting) |
| Small \(\lambda\)                   | Moderate        | Moderate | Moderate | Slight Overfitting |
| Large \(\lambda\)                    | Low             | High | Low      | Underfitting |

**Regularization reduces overfitting** by decreasing **model complexity** and **variance**, while slightly **increasing bias**.

---

# Mathematical Intuitions about Regularization

### 1. Model Complexity in Linear Models

In a **linear model**, predictions are given by:

\[
y = w_1 x_1 + w_2 x_2 + \dots + w_d x_d + b
\]

- If the **weights** (*w*) are **small**, the model's response to changes in input is gradual and smooth.
- If the **weights** are **large**, the model becomes highly sensitive to small changes in input, allowing it to fit noise in the data.

#### Example: Small vs. Large Weights

Imagine we are modeling a function using a linear model.

- **Small Weights** (\(w_1 = 0.5, w_2 = -0.3\)):
  - The model makes **gradual** changes in predictions as inputs change.
  - Predictions are stable and generalize well.

- **Large Weights** (\(w_1 = 100, w_2 = -150\)):
  - Even a **tiny** change in input can cause a **huge** change in output.
  - The model can memorize the training data exactly, but it won’t generalize well.

Thus, **large weights increase model sensitivity**, making the model capable of capturing noise, which leads to overfitting.

### 2. Why Does Overfitting Happen with Large Weights?

Overfitting means that the model fits the training data **too well**, capturing not just the underlying pattern but also the noise. Large weights allow overfitting because:

1. **Large Weights Create Sharp Decision Boundaries**
   - In classification problems, large weights create **very steep decision boundaries**.
   - The model adjusts excessively to training points, leading to poor generalization.

2. **Large Weights Exaggerate the Effect of Small Changes**
   - If weights are large, even a **small fluctuation in input values** leads to **large fluctuations in output**.
   - This means that small variations (noise) in training data get amplified, making the model highly unstable.

3. **Large Weights Increase Variance**
   - Variance measures how much the model’s predictions change with different training sets.
   - **Large weights make the model more sensitive to training data**, meaning its predictions change drastically with different datasets.
   - This leads to high variance and poor generalization.

### 3. Mathematical View: Why Large Weights Increase Complexity

We can measure model complexity by looking at the **derivatives of the function**:

\[
h(x) = w_1 x + w_2 x^2 + w_3 x^3 + \dots
\]

The derivative tells us how **fast the output changes** when the input changes:

\[
\frac{d}{dx} h(x) = w_1 + 2w_2 x + 3w_3 x^2 + \dots
\]

- If **weights are small**, the derivative is small → **smooth, simple function**.
- If **weights are large**, the derivative is large → **sharp, complex function**.

Thus, **large weights allow the model to respond more aggressively to input changes, leading to high complexity**.

---

# L2 Regularization (Ridge Regression)
### 1. The Loss Function in Ridge Regression

The objective in Ridge Regression is to minimize the following cost function:

\[
J(w) = \sum_n (y_n - w^T x_n - b)^2 + \lambda ||w||_2^2
\]

Where:

- The first term \(\sum_n (y_n - w^T x_n - b)^2\) is the **usual squared error loss** (captures how well the model fits the data).
- The second term \(\lambda ||w||_2^2\) is the **L2 penalty** (regularization term), which is equal to:

  \[
  ||w||_2^2 = \sum_d w_d^2
  \]

- The hyperparameter \(\lambda\) controls the **strength of regularization**.

### 2. Why Does L2 Regularization Shrink \(w\)?

Since our goal is to minimize \(J(w)\), we must find the values of \(w\) that make \(J(w)\) as small as possible.

##### (A) Without Regularization

In standard least squares regression, we only minimize:

\[
J(w) = \sum_n (y_n - w^T x_n - b)^2
\]

- The best \(w\) is chosen **only based on data fit**.
- If features are highly correlated or data is noisy, \(w\) can become **very large** to perfectly fit the training data.

##### (B) With Regularization

Now, in Ridge Regression, we also minimize:

\[
J(w) = \sum_n (y_n - w^T x_n - b)^2 + \lambda \sum_d w_d^2
\]

Since we are minimizing the **sum of squared weights**, this forces \(w\) to be small. Why?

1. **Convexity of \( ||w||^2 \)**
   - The function \( ||w||_2^2 = \sum w_d^2 \) is **convex**.
   - When we minimize \(J(w)\), we are also minimizing \( ||w||_2^2 \), namely minimizing \(\sum w_d^2 \).
   - This naturally leads to **small weight values**.

2. **Trade-off Between Fit and Simplicity**
   - The model **wants to reduce both error and weight size**.
   - If \(\lambda\) is **small**, the model allows some **large** weights.
   - If \(\lambda\) is **large**, the model prefers **small** weights even if it slightly increases prediction error.

##### (C) Gradient View

To minimize \(J(w)\), we compute its derivative:

\[
\frac{d}{dw} J(w) = \frac{d}{dw} \left[ ||y - Xw||^2 + \lambda ||w||^2 \right]
\]

\[
\nabla J(w) = 2X^T (Xw - y) + 2\lambda w
\]

Setting this to zero:

\[
2X^T Xw - 2X^T y + 2\lambda w = 0
\]

Solving for \(w\):

\[
w = (X^T X + \lambda I)^{-1} X^T y
\]

- The extra term \(\lambda I\) **shrinks the weights**.
- If \(\lambda\) increases, the weights \(w\) decrease further.

---

## Trade-off of \( \lambda \)

* Use **Cross-Validation** to Find the Best \( \lambda \).
* **Small \( \lambda \)** → Complex model, good for large datasets. 
* **Large \( \lambda \)** → Simpler model, good for small/noisy datasets.


#### 1. Effect of Small \( \lambda \) (Near Zero)

##### Why It Leads to a Complex Model?
- When \( \lambda \approx 0 \), the Ridge Regression cost function:

\[
J(w) = \sum (y_i - h(x_i))^2 + \lambda \sum w_j^2
\]

becomes almost identical to **Ordinary Least Squares (OLS)**:

\[
J(w) = \sum (y_i - h(x_i))^2
\]

- The model **minimizes only the error term**, without penalizing large weights.
- As a result:
  - The model can learn **very complex patterns**.
  - The weights \( w_j \) **can take large values**, leading to **high variance**.
  - The model may **overfit** the training data.

##### Why is this Good for Large Datasets?
- A large dataset provides **enough examples** to generalize complex patterns.
- The risk of overfitting **decreases** when \( N \) (training samples) is large.
- Example: **Deep learning models** often work well because they train on millions of examples.

✅ Small \( \lambda \) → **More flexibility, good for large datasets with enough examples to prevent overfitting.**  
❌ Small \( \lambda \) → **Overfits if dataset is small or noisy.**

#### 2. Effect of Large \( \lambda \)

##### Why It Leads to a Simpler Model?
- When \( \lambda \) is **large**, the penalty term \( \lambda \sum w_j^2 \) dominates.
- This forces **all weights \( w_j \) to be small**.
- If \( \lambda \to \infty \), all \( w_j \to 0 \), making the model **nearly constant**.
- The model **ignores fine details** and captures only **general trends**.

##### Why is this Good for Small/Noisy Datasets?
- In **small datasets**, we don’t have enough samples to learn complex relationships.
- In **noisy datasets**, reducing model complexity prevents overfitting to noise.
- Large \( \lambda \) forces the model to focus only on **strong signals**, ignoring weak/noisy patterns.

✅ Large \( \lambda \) → **Reduces variance, prevents overfitting in small/noisy datasets.**  
❌ Large \( \lambda \) → **May underfit and miss important patterns.**

#### 3. Balancing Bias and Variance

| \( \lambda \) Value | Effect on Model Complexity | Suitability |
|----------------|--------------------------|-------------|
| **Small \( \lambda \) (\(\approx 0\))** | **Complex, high variance** (captures details but may overfit) | Large datasets with enough data to generalize |
| **Optimal \( \lambda \) (Cross-Validated)** | **Balanced bias-variance tradeoff** | Best generalization (avoids overfitting and underfitting) |
| **Large \( \lambda \) ( \( \to \infty \) )** | **Simple, high bias** (ignores details, may underfit) | Small datasets or noisy data where generalization is key |




